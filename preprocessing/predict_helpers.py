import pandas as pd
import numpy as np
from datetime import timedelta
from geopy.distance import geodesic
from preprocessing.scaling import apply_feature_scalers_from_saved
from api.get_meteo import get_meteo_data

def preprocess_input_data(df: pd.DataFrame) -> pd.DataFrame:
    #df = get_meteo_data(df)
    #df = df.drop(columns=['depth', 'latitude', 'longitude', 'time', 'index'])
    print("==> –ò—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:", df.columns)
    df = df.rename(columns={
        'mlotst_cglo': 'mlotst',
        'siconc_cglo': 'siconc',
        'sithick_cglo': 'sithick',
        'so_cglo': 'so',
        'thetao_cglo': 'thetao',
        'uo_cglo': 'uo',
        'vo_cglo': 'vo',
        'so_cglo': 'so',
        'zos_cglo': 'zos'
    })
    #df.fillna(0, inplace=True)  # –ò–∑–º–µ–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    for column in df.select_dtypes(include=['float64', 'int64']).columns: 
        df[column] = df[column].fillna(df[column].mean())

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω—ã
    df["timestamp"] = pd.to_datetime(df["timestamp"]).dt.tz_localize(None)

    # === –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
    df["hour"] = df["timestamp"].dt.hour
    df["dayofweek"] = df["timestamp"].dt.dayofweek
    df["month"] = df["timestamp"].dt.month
    #df["season"] = df["month"].map(lambda x: (x % 12 + 3) // 3)
    print("==> –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è season:", df.columns)

    #  === –ì–ï–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò ===
    def haversine(lat1, lon1, lat2, lon2):
        return geodesic((lat1, lon1), (lat2, lon2)).km

    df["lat_diff"] = df["lat"].diff().fillna(0)
    df["lon_diff"] = df["lon"].diff().fillna(0)
    df["course_diff"] = df["course"].diff().fillna(0)
    df["distance_to_destination"] = df.apply(
    lambda row: haversine(row["lat"], row["lon"], row["lat_destination"], row["lon_destination"]), axis=1)
    df["log_distance"] = np.log1p(df["distance_to_destination"])  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ

    # === –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò ===
    df["speed_diff"] = df["speed"].diff().fillna(0)
    df["acceleration"] = df["speed_diff"].diff().fillna(0)
    df["bearing_change"] = df["course"].diff().fillna(0)
    df["moving"] = (df["speed"] > 0).astype(int)

    # === –û–ë–†–ê–ë–û–¢–ö–ê –í–´–ë–†–û–°–û–í ===
    #df = df[df["speed"] < df["speed"].quantile(0.99)]

    # === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í ===
    num_cols = ["speed", "course", "lat_diff", "lon_diff", "course_diff", "log_distance", "speed_diff", "acceleration", "bearing_change"]
    # === –ú–ï–¢–ï–û–ü–†–ò–ó–ù–ê–ö–ò ===
    meteo_cols = [col for col in df.columns if any(x in col for x in ["mlotst", "siconc", "sithick", "so", "thetao", "uo", "vo", "zos"])]
    df = apply_feature_scalers_from_saved(df, num_cols, meteo_cols)
    print("==> –ü–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è:", df.columns)

    return df

import os
import joblib
import pandas as pd
import numpy as np
import tensorflow as tf
import xgboost as xgb
from datetime import timedelta

def predict_eta_from_new_data(df_new, seq_length=10, model_dir='models'):
    """
    –ü—Ä–æ–≥–Ω–æ–∑ ETA_diff –∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ ETA –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å—É–¥–Ω–∞.

    –í—Å–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ model_dir.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - ETA_diff (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    - ETA (datetime: timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏ + ETA_diff)
    """
    print("==> –ö–æ–ª–æ–Ω–∫–∏ –Ω–∞ –≤—Ö–æ–¥–µ –≤ predict:", df_new.columns)
    if 'timestamp' not in df_new.columns:
        raise ValueError("–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'timestamp'.")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º feature_cols
    #feature_cols = joblib.load("models/feature_cols.pkl")
    #print("==> Feature columns –∏–∑ –º–æ–¥–µ–ª–∏:", feature_cols)
    feature_cols = ['lat', 'lon', 'speed', 'course', 'lat_diff', 'lon_diff', 'course_diff', 'log_distance', 'speed_diff', 'acceleration', 'bearing_change', 'moving', 'mlotst', 'siconc', 'sithick', 'so', 'thetao', 'uo', 'vo', 'zos']
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä—ã
    scaler = joblib.load("models/X_scaler.pkl")
    label_scaler = joblib.load("models/label_scaler.pkl")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    lstm_feature_extractor = tf.keras.models.load_model("models/lstm_feature_extractor.keras")

    xgb_model = xgb.XGBRegressor()
    xgb_model.load_model("models/xgb_model.json")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    df_sorted = df_new.sort_values(by="timestamp")

    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ seq_length —Å—Ç—Ä–æ–∫
    import streamlit as st
    print("üß† df_sorted columns:", df_sorted.columns.tolist())
    st.text(f"üß† df_sorted columns:\n{df_sorted.columns.tolist()}")
    print("üìå feature_cols from model:", feature_cols)
    st.text(f"üìå feature_cols from model:\n{feature_cols}")

    seq_df = df_sorted[feature_cols].tail(seq_length)
    print("==> –ö–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª—å—é:", seq_df.columns)

    if len(seq_df) < seq_length:
        raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {seq_length}, –∞ –ø–æ–ª—É—á–µ–Ω–æ {len(seq_df)}")

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaled_seq = scaler.transform(seq_df.values)
    input_data = scaled_seq.reshape(1, seq_length, len(feature_cols))

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ LSTM
    lstm_features = lstm_feature_extractor.predict(input_data, verbose=0)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ETA_diff (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    pred_scaled = xgb_model.predict(lstm_features.reshape(1, -1))
    eta_diff_seconds = float(label_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()[0])

    # ETA –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏
    last_time = df_sorted["timestamp"].iloc[-1]
    eta = last_time + timedelta(seconds=eta_diff_seconds)

    return {
        "eta_diff_seconds": eta_diff_seconds,
        "eta": eta,
        "base_time": last_time
    }
