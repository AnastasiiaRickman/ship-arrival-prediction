import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error
import xgboost as xgb
import joblib
import os

from preprocessing.scaling import fit_feature_scalers
from training.sequence_builder import create_sequences
from preprocessing.load_and_clean import load_and_prepare_data

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
import requests
import os
import matplotlib.pyplot as plt
def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    plt.figure(figsize=(12, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ MAE (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    if 'mae' in history.history:
        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'], label='Train MAE')
        plt.plot(history.history['val_mae'], label='Validation MAE')
        plt.title('MAE over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig('artifacts/training_history.png')  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plt.show()

# API_URL = "http://127.0.0.1:8000"
LOCAL_DATA_DIR = "data"  # –≠—Ç–∞ –ø–∞–ø–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ CSV

# os.makedirs(LOCAL_DATA_DIR, exist_ok=True)

# # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å API
# response = requests.get(f"{API_URL}/files")
# csv_files = response.json()

# # –°–∫–∞—á–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π CSV
# for filename in csv_files:
#     file_url = f"{API_URL}/files/{filename}"
#     file_path = os.path.join(LOCAL_DATA_DIR, filename)

#     r = requests.get(file_url)
#     if r.status_code == 200:
#         with open(file_path, 'wb') as f:
#             f.write(r.content)
#         print(f"‚úÖ –§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω.")
#     else:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}")

# # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –ø–∞–π–ø–ª–∞–π–Ω

df = load_and_prepare_data(LOCAL_DATA_DIR)

# === 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
num_cols = ["speed", "course", "lat_diff", "lon_diff", "course_diff",
            "log_distance", "speed_diff", "acceleration", "bearing_change"]

meteo_cols = [col for col in df.columns if any(x in col for x in ["mlotst", "siconc", "sithick", "so", "thetao", "uo", "vo", "zos"])]

df, num_scaler, meteo_scaler = fit_feature_scalers(df, num_cols, meteo_cols)

# === 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ===
baseline = ["speed", "lat", "lon", "distance_to_destination"]
geo = ["lat_diff", "lon_diff", "course_diff", "log_distance"]
temporal = ["hour", "dayofweek", "month", "season"]
dynamic = ["speed_diff", "acceleration", "bearing_change", "moving"]
meteo = [col for col in df.columns if any(x in col for x in ["mlotst", "siconc", "sithick", "so", "thetao", "uo", "vo", "zos"])]
synthetic = ["log_distance"]

feature_cols = baseline + geo + temporal + dynamic + synthetic
joblib.dump(feature_cols, 'models/feature_cols.pkl')
dataset, labels = create_sequences(df[feature_cols].values, df["ETA_diff"].values, seq_length=10)

# === 4. –î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ ===
X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)

# === 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===
X_scaler = MinMaxScaler()
X_train = X_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = X_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
joblib.dump(X_scaler, 'models/X_scaler.pkl')
label_scaler = StandardScaler()
y_train = label_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test = label_scaler.transform(y_test.reshape(-1, 1)).flatten()
joblib.dump(label_scaler, 'models/label_scaler.pkl')

# === 6. LSTM –º–æ–¥–µ–ª—å ===

# lstm_model = tf.keras.Sequential([
#     tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2])),  # üëà —Ç–µ–ø–µ—Ä—å input –∑–¥–µ—Å—å
#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.LSTM(64, return_sequences=True),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.LSTM(32),
#     tf.keras.layers.Dropout(0.1),
#     tf.keras.layers.Dense(32, activation='relu'),
#     tf.keras.layers.Dense(16, activation='relu'),
#     tf.keras.layers.Dense(1)
# ])
lstm_model = tf.keras.Sequential([
    tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2])),  # üëà —Ç–µ–ø–µ—Ä—å input –∑–¥–µ—Å—å
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.LSTM(32, return_sequences=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(1)  # Feature —Å–ª–æ–π
])

lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='mse')

# callbacks = [
#     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
#     tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)
# ]
# === 6. LSTM Feature Extractor ===

# lstm_model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
print("üß† –û–±—É—á–µ–Ω–∏–µ LSTM...")
history = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=32,  # –ú–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 16
    # callbacks=callbacks,
    verbose=1  # –î–ª—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ª–æ–≥–æ–≤
)

# –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è LSTM
plot_training_history(history)

# –°–æ–∑–¥–∞–µ–º Feature Extractor –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))
x = lstm_model.layers[0](inputs)  # Bidirectional LSTM
x = lstm_model.layers[1](x)       # Dropout
x = lstm_model.layers[2](x)       # LSTM
x = lstm_model.layers[3](x)       # Dropout
x = lstm_model.layers[4](x)       # Dense(32)
x = lstm_model.layers[5](x)       # Dense(16)
feature_output = lstm_model.layers[6](x)  # Dense(8) ‚Äî –Ω–∞—à —Å–ª–æ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

# # –°–û–ó–î–ê–ù–ò–ï FEATURE EXTRACTOR (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏)
feature_extractor = tf.keras.Model(inputs=inputs, outputs=feature_output)

print("üìê –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
X_train_features = feature_extractor.predict(X_train)
X_test_features = feature_extractor.predict(X_test)

#=== 8. XGBoost ===
# xgb_model = xgb.XGBRegressor(
#     max_depth=10, 
#     learning_rate=0.18907365925970332,
#     n_estimators=362, 
#     subsample=0.9183945866232028, 
#     colsample_bytree=0.9069336171226844,
#     gamma=0.7367508381925425, 
#     reg_alpha=0.8747927173857223,
#     reg_lambda=0.3767252935537126,
#     objective='reg:squarederror'
# )

xgb_model = xgb.XGBRegressor(
    max_depth=6,
    learning_rate=0.05,
    n_estimators=500,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=1,
    reg_alpha=0.3,
    reg_lambda=1.0
)

print("–û–±—É—á–µ–Ω–∏–µ XGBoost...")
xgb_model.fit(X_train_features, y_train)
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost
plt.figure(figsize=(10, 6))
xgb.plot_importance(xgb_model)
plt.title('Feature Importance')
plt.tight_layout()
plt.savefig('artifacts/xgb_feature_importance.png')
plt.show()

y_pred = xgb_model.predict(X_test_features)
y_pred_original = label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_original = label_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

print("üìä MAE (scaled):", mean_absolute_error(y_test, y_pred))
print("üìä MAE (original):", mean_absolute_error(y_test_original, y_pred_original))

# === 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
os.makedirs("models", exist_ok=True)
feature_extractor.save("models/lstm_feature_extractor.keras")
xgb_model.save_model("models/xgb_model.json")

print("‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ!")
print("Feature columns –∏–∑ –º–æ–¥–µ–ª–∏:", feature_cols)
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")

os.makedirs("artifacts", exist_ok=True)

np.save("artifacts/X_train_features.npy", X_train_features)
np.save("artifacts/X_test_features.npy", X_test_features)
np.save("artifacts/y_train.npy", y_train)
np.save("artifacts/y_test.npy", y_test)
joblib.dump(label_scaler, "artifacts/label_scaler.pkl")