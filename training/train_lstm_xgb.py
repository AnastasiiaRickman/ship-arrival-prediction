import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error
import xgboost as xgb
import joblib
import os

from preprocessing.scaling import fit_feature_scalers
from training.sequence_builder import create_sequences
from preprocessing.load_and_clean import load_and_prepare_data

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
import requests
import os
import matplotlib.pyplot as plt
def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    plt.figure(figsize=(12, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ MAE (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    if 'mae' in history.history:
        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'], label='Train MAE')
        plt.plot(history.history['val_mae'], label='Validation MAE')
        plt.title('MAE over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig('artifacts/training_history.png')  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plt.show()

# API_URL = "http://127.0.0.1:8000"
LOCAL_DATA_DIR = "data"  # –≠—Ç–∞ –ø–∞–ø–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ CSV

# os.makedirs(LOCAL_DATA_DIR, exist_ok=True)

# # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å API
# response = requests.get(f"{API_URL}/files")
# csv_files = response.json()

# # –°–∫–∞—á–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π CSV
# for filename in csv_files:
#     file_url = f"{API_URL}/files/{filename}"
#     file_path = os.path.join(LOCAL_DATA_DIR, filename)

#     r = requests.get(file_url)
#     if r.status_code == 200:
#         with open(file_path, 'wb') as f:
#             f.write(r.content)
#         print(f"‚úÖ –§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω.")
#     else:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}")

# # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –ø–∞–π–ø–ª–∞–π–Ω

df = load_and_prepare_data(LOCAL_DATA_DIR)

# === 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
num_cols = ["speed", "course", "lat_diff", "lon_diff", "course_diff",
            "log_distance", "speed_diff", "acceleration", "bearing_change"]

df, num_scaler = fit_feature_scalers(df, num_cols)

# === 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ===
baseline = ["speed", "lat", "lon", "distance_to_destination"]
geo = ["lat_diff", "lon_diff", "course_diff", "log_distance"]
temporal = ["hour", "dayofweek", "month", "season"]
dynamic = ["speed_diff", "acceleration", "bearing_change", "moving"]
synthetic = ["log_distance"]

#df, num_scaler, robust_scaler = fit_feature_scalers(df, num_cols, synthetic, temporal)

feature_cols = baseline + geo + temporal + dynamic + synthetic
joblib.dump(feature_cols, 'models/feature_cols.pkl')
# —Ç—É—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–µ—Ç–∫—É!!!
dataset, labels = create_sequences(df[feature_cols].values, df["ETA_diff"].values, seq_length=10)
_, distances = create_sequences(
    df[feature_cols].values,
    df["distance_to_destination"].values,
    seq_length=10
)

# === 4. –î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ ===

X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)

# === 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===

from sklearn.preprocessing import RobustScaler
# X_scaler = RobustScaler(quantile_range=(5, 95))
X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = X_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
joblib.dump(X_scaler, 'models/X_scaler.pkl')
label_scaler = StandardScaler()
y_train = label_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test = label_scaler.transform(y_test.reshape(-1, 1)).flatten()
joblib.dump(label_scaler, 'models/label_scaler.pkl')

from tensorflow.keras import layers, regularizers

def build_feature_lstm_model(input_shape, num_heads=4, key_dim=64, dropout_rate=0.25):
    inputs = layers.Input(shape=input_shape)
    
    # –í—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
    x = layers.Dense(64, activation='swish')(inputs)
    
    # –ü–µ—Ä–≤—ã–π BiLSTM + Residual
    lstm1 = layers.Bidirectional(
        layers.LSTM(96, return_sequences=True,
                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))
    )(x)
    lstm1 = layers.LayerNormalization()(lstm1)
    
    # –í—Ç–æ—Ä–æ–π BiLSTM
    lstm2 = layers.Bidirectional(
        layers.LSTM(64, return_sequences=True)
    )(lstm1)
    lstm2 = layers.LayerNormalization()(lstm2)

    # Attention
    attention = layers.MultiHeadAttention(
        num_heads=num_heads,
        key_dim=key_dim,
        dropout=dropout_rate
    )(query=lstm2, value=lstm2)

    # Residual connection
    residual = layers.Add()([lstm2, attention])

    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    x = layers.GlobalAveragePooling1D()(residual)
    x = layers.Dense(128, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(dropout_rate)(x)

    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è XGBoost
    features = layers.Dense(64, activation='swish', name='features_for_xgb')(x)

    # üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    output = layers.Dense(1, name='regression_output')(features)
    
    return tf.keras.Model(inputs=inputs, outputs=output)


callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        min_delta=0.001,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'best_model.keras',
        save_best_only=True,
        monitor='val_loss'
    )
]

optimizer = tf.keras.optimizers.AdamW(
    learning_rate=0.001,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    weight_decay=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=True
)

def hybrid_loss(y_true, y_pred):
    # MSE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))
    
    # MAE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å –≤–µ—Å–∞–º–∏
    abs_error = tf.abs(y_true - y_pred)
    weights = tf.where(y_true < 1.0, 3.0, 1.0)
    mae_loss = tf.reduce_mean(weights * abs_error)
    
    return 0.7 * mse_loss + 0.3 * mae_loss

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

# –ü–æ—Å—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å
lstm_model = build_feature_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))

# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
lstm_model.compile(
    optimizer=optimizer,
    loss=hybrid_loss,
    metrics=['mae']
)

# –û–±—É—á–∞–µ–º
history = lstm_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=300,
    batch_size=64,
    callbacks=callbacks,
    verbose=1
)

# –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è LSTM
plot_training_history(history)

# –ú–æ–¥–µ–ª—å, –∏–∑–≤–ª–µ–∫–∞—é—â–∞—è –ø—Ä–∏–∑–Ω–∞–∫–∏
feature_extractor = tf.keras.Model(
    inputs=lstm_model.input,  # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏
    outputs=lstm_model.get_layer('features_for_xgb').output  # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
)
X_train_features = feature_extractor.predict(X_train)
X_test_features = feature_extractor.predict(X_test)


# === 8. XGBoost ===
xgb_model = xgb.XGBRegressor(
    max_depth=6, 
    learning_rate=0.08654969092291069,
    n_estimators=496, 
    subsample=0.8648324782679198, 
    colsample_bytree=0.7177686830730101,
    gamma=0.005168380231533848, 
    reg_alpha=0.49457023434877395,
    reg_lambda=0.30214603419143765,
    objective='reg:squarederror'
)

# xgb_model = xgb.XGBRegressor(
#     max_depth=6,
#     learning_rate=0.05,
#     n_estimators=500,
#     subsample=0.8,
#     colsample_bytree=0.8,
#     gamma=1,
#     reg_alpha=0.3,
#     reg_lambda=1.0
# )

# –û–±—É—á–µ–Ω–∏–µ XGBoost
print("–û–±—É—á–µ–Ω–∏–µ XGBoost...")
xgb_model.fit(X_train_features, y_train)

def weighted_mae(y_true, y_pred, eps=1e-6):
    weights = 1 / (np.abs(y_true) + eps)  # –û—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Å
    weights = np.where(y_true < 3600, 1.0, weights)  # –î–ª—è —Ä–µ–π—Å–æ–≤ <1 —á–∞—Å–∞ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤–µ—Å=1
    return np.mean(weights * np.abs(y_pred - y_true))

def relative_mae(y_true, y_pred, eps=1e-6):
    return np.mean(np.abs(y_pred - y_true) / (np.abs(y_true) + eps))


# –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
y_pred = xgb_model.predict(X_test_features)
y_pred_original = label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_original = label_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

# –û—Ü–µ–Ω–∫–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ (–¥–Ω–∏)
print("üìä MAE (original):", mean_absolute_error(y_test_original, y_pred_original))
print("üìä Weighted MAE (original):", weighted_mae(y_test_original, y_pred_original))
print("üìä Relative MAE (original):", relative_mae(y_test_original, y_pred_original) * 100, "%")

# –û—Ü–µ–Ω–∫–∞ –≤ scaled-–º–∞—Å—à—Ç–∞–±–µ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
print("üìä MAE (scaled):", mean_absolute_error(y_test, y_pred))
print("üìä Weighted MAE (scaled):", weighted_mae(y_test, y_pred))
print("üìä Relative MAE (scaled):", relative_mae(y_test, y_pred) * 100, "%")


# === 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
os.makedirs("models", exist_ok=True)
feature_extractor.save("models/lstm_feature_extractor.keras")
xgb_model.save_model("models/xgb_model.json")

print("‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ!")
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")

os.makedirs("artifacts", exist_ok=True)

np.save("artifacts/X_train_features.npy", X_train_features)
np.save("artifacts/X_test_features.npy", X_test_features)
np.save("artifacts/y_train.npy", y_train)
np.save("artifacts/y_test.npy", y_test)
joblib.dump(label_scaler, "artifacts/label_scaler.pkl")