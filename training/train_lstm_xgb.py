import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
import xgboost as xgb
import joblib
import os

from preprocessing.scaling import fit_feature_scalers
from training.sequence_builder import create_sequences
from utils.save_load import save_models
from preprocessing.load_and_clean import load_and_prepare_data

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
import requests
import os

API_URL = "http://127.0.0.1:8000"
LOCAL_DATA_DIR = "data"  # –≠—Ç–∞ –ø–∞–ø–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ CSV

os.makedirs(LOCAL_DATA_DIR, exist_ok=True)

# # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å API
# response = requests.get(f"{API_URL}/files")
# csv_files = response.json()

# # –°–∫–∞—á–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π CSV
# for filename in csv_files:
#     file_url = f"{API_URL}/files/{filename}"
#     file_path = os.path.join(LOCAL_DATA_DIR, filename)

#     r = requests.get(file_url)
#     if r.status_code == 200:
#         with open(file_path, 'wb') as f:
#             f.write(r.content)
#         print(f"‚úÖ –§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω.")
#     else:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}")

# –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –ø–∞–π–ø–ª–∞–π–Ω
from preprocessing.load_and_clean import load_and_prepare_data

df = load_and_prepare_data(LOCAL_DATA_DIR)

# === 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
num_cols = ["speed", "course", "lat_diff", "lon_diff", "course_diff",
            "log_distance", "speed_diff", "acceleration", "bearing_change"]

meteo_cols = [col for col in df.columns if any(x in col for x in ["mlotst", "siconc", "sithick", "so", "thetao", "uo", "vo", "zos"])]

df, num_scaler, meteo_scaler = fit_feature_scalers(df, num_cols, meteo_cols)

# === 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ===
feature_cols = ["lat", "lon"] + num_cols + ["moving"] + meteo_cols
joblib.dump(feature_cols, 'models/feature_cols.pkl')
dataset, labels = create_sequences(df[feature_cols].values, df["ETA_diff"].values, seq_length=10)

# === 4. –î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ ===
X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)

# === 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===
X_scaler = MinMaxScaler()
X_train = X_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = X_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
joblib.dump(X_scaler, 'models/X_scaler.pkl')
label_scaler = MinMaxScaler()
y_train = label_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test = label_scaler.transform(y_test.reshape(-1, 1)).flatten()
joblib.dump(label_scaler, 'models/label_scaler.pkl')

# === 6. LSTM –º–æ–¥–µ–ª—å ===

lstm_model = tf.keras.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.LSTM(32, return_sequences=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mse')

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)

print("üß† –û–±—É—á–µ–Ω–∏–µ LSTM...")
lstm_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=16,
    callbacks=[early_stop]
)

# === –°–û–ó–î–ê–ù–ò–ï FEATURE EXTRACTOR (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏) ===
inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))
x = lstm_model.layers[0](inputs)  # Bidirectional LSTM
x = lstm_model.layers[1](x)       # Dropout
x = lstm_model.layers[2](x)       # LSTM
x = lstm_model.layers[3](x)       # Dropout
x = lstm_model.layers[4](x)       # Dense(32)
x = lstm_model.layers[5](x)       # Dense(16)
feature_output = lstm_model.layers[6](x)  # Dense(8) ‚Äî –Ω–∞—à —Å–ª–æ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

feature_extractor = tf.keras.Model(inputs=inputs, outputs=feature_output)

print("üìê –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
X_train_features = feature_extractor.predict(X_train)
X_test_features = feature_extractor.predict(X_test)
# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º—É –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print(X_train_features.shape)  # –ù–∞–ø—Ä–∏–º–µ—Ä, (batch_size, seq_length, features)
print(X_test_features.shape)

#=== 8. XGBoost ===
xgb_model = xgb.XGBRegressor(
    max_depth=7,
    learning_rate=0.18,
    n_estimators=362,
    subsample=0.9,
    colsample_bytree=0.9,
    gamma=0.008,
    reg_alpha=0.23,
    reg_lambda=0.37,
    objective='reg:squarederror'
)

print("üöÄ –û–±—É—á–µ–Ω–∏–µ XGBoost...")
xgb_model.fit(X_train_features, y_train)

y_pred = xgb_model.predict(X_test_features)
y_pred_original = label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_original = label_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

print("üìä MAE (scaled):", mean_absolute_error(y_test, y_pred))
print("üìä MAE (original):", mean_absolute_error(y_test_original, y_pred_original))

# === 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
os.makedirs("models", exist_ok=True)
feature_extractor.save("models/lstm_feature_extractor.keras")
xgb_model.save_model("models/xgb_model.json")

print("‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ!")
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")

os.makedirs("artifacts", exist_ok=True)

np.save("artifacts/X_train_features.npy", X_train_features)
np.save("artifacts/X_test_features.npy", X_test_features)
np.save("artifacts/y_train.npy", y_train)
np.save("artifacts/y_test.npy", y_test)
joblib.dump(label_scaler, "artifacts/label_scaler.pkl")

