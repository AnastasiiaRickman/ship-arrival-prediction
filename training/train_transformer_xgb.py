# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
import requests
import os
import matplotlib.pyplot as plt  # üî• –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤

API_URL = "http://127.0.0.1:8000"
LOCAL_DATA_DIR = "data"  # –≠—Ç–∞ –ø–∞–ø–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ CSV

os.makedirs(LOCAL_DATA_DIR, exist_ok=True)

from preprocessing.load_and_clean import load_and_prepare_data

df = load_and_prepare_data(LOCAL_DATA_DIR)

# === –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ETA_diff (–¥–æ–±–∞–≤–ª–µ–Ω–æ –∑–¥–µ—Å—å) ===
print("=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ETA_diff ===")
print("–ú–∏–Ω. (—Å–µ–∫):", df["ETA_diff"].min())
print("–ú–∞–∫—Å. (—Å–µ–∫):", df["ETA_diff"].max())
print("–°—Ä–µ–¥–Ω–µ–µ (–º–∏–Ω):", df["ETA_diff"].mean() / 60)
print("–ú–µ–¥–∏–∞–Ω–∞ (–º–∏–Ω):", df["ETA_diff"].median() / 60)
print("90-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (–º–∏–Ω):", df["ETA_diff"].quantile(0.9) / 60)
print("95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (–º–∏–Ω):", df["ETA_diff"].quantile(0.95) / 60)

plt.figure(figsize=(10, 4))
plt.hist(df["ETA_diff"], bins=100, color='skyblue', edgecolor='black')
plt.title("ETA_diff (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)")
plt.xlabel("–°–µ–∫—É–Ω–¥—ã")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 4))
plt.hist(df["ETA_diff"] / 60, bins=100, color='lightgreen', edgecolor='black')
plt.title("ETA_diff (–≤ –º–∏–Ω—É—Ç–∞—Ö)")
plt.xlabel("–ú–∏–Ω—É—Ç—ã")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 4))
plt.hist(df["ETA_diff"] / 3600, bins=100, color='salmon', edgecolor='black')
plt.title("ETA_diff (–≤ —á–∞—Å–∞—Ö)")
plt.xlabel("–ß–∞—Å—ã")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
plt.grid(True)
plt.show()
# === –∫–æ–Ω–µ—Ü –≤—Å—Ç–∞–≤–∫–∏ ===

# === 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
num_cols = ["speed", "course", "lat_diff", "lon_diff", "course_diff",
            "log_distance", "speed_diff", "acceleration", "bearing_change"]

meteo_cols = [col for col in df.columns if any(x in col for x in ["mlotst", "siconc", "sithick", "so", "thetao", "uo", "vo", "zos"])]

from preprocessing.scaling import fit_feature_scalers
df, num_scaler, meteo_scaler = fit_feature_scalers(df, num_cols, meteo_cols)

# === 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ===
feature_cols = ["lat", "lon"] + num_cols + ["moving"] + meteo_cols
import joblib
joblib.dump(feature_cols, 'models/feature_cols.pkl')

from training.sequence_builder import create_sequences
dataset, labels = create_sequences(df[feature_cols].values, df["ETA_diff"].values, seq_length=10)

# === 4. –î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ ===
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)

# === 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
X_scaler = MinMaxScaler()
X_train = X_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = X_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
joblib.dump(X_scaler, 'models/X_scaler.pkl')

label_scaler = RobustScaler()
y_train = label_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test = label_scaler.transform(y_test.reshape(-1, 1)).flatten()
joblib.dump(label_scaler, 'models/label_scaler.pkl')

# === 6. –ú–æ–¥–µ–ª—å ===
from keras import regularizers, layers, models
import tensorflow as tf

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)
    ff = layers.Dense(ff_dim, activation="relu", kernel_regularizer=regularizers.l2(1e-4))(x)
    ff = layers.Dropout(dropout)(ff)
    ff = layers.Dense(inputs.shape[-1], kernel_regularizer=regularizers.l2(1e-4))(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

def build_stronger_transformer(input_shape,
                                head_size=64,
                                num_heads=4,
                                ff_dim=128,
                                num_blocks=2,
                                mlp_units=[128, 64],
                                dropout=0.2):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)
    x = layers.GlobalMaxPooling1D()(x)
    for units in mlp_units:
        x = layers.Dense(units, activation="relu", kernel_regularizer=regularizers.l2(1e-4))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout)(x)
    output = layers.Dense(1)(x)
    model = models.Model(inputs, output)
    return model

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

print("üß† –û–±—É—á–µ–Ω–∏–µ Transformer...")
model = build_stronger_transformer(input_shape=(X_train.shape[1], X_train.shape[2]))
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop]
)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
y_pred_original = label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_test_original = label_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

print("üìä MAE (scaled):", mean_absolute_error(y_test, y_pred))
print("üìä MAE (original):", mean_absolute_error(y_test_original, y_pred_original))

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
import os
os.makedirs("models", exist_ok=True)

print("‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ!")
